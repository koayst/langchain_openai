{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be492c30-62ad-4e10-9295-e19f1aa033b8",
   "metadata": {},
   "source": [
    "# Generative AI - LangChain x OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9668c1dd-1dc5-4dbd-94ed-01366d0f4778",
   "metadata": {},
   "source": [
    "<img src=\"langchain_n_openai.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e054e2df-f740-4965-b7e2-a76f4ccd89e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# once you have installed the libraries, you can comment it back\n",
    "# you only need to run this once\n",
    "\n",
    "#!pip install arxiv chroma docarray duckduckgo-search langchain numexpr openai pydantic \n",
    "#!pip install pypdfium2 python-dotenv pytube tiktoken youtube-transcript-api wikipedia\n",
    "\n",
    "# arxiv                  = 2.0.0\n",
    "# Chroma                 = 0.4.18\n",
    "# duckduckgo-search      = 3.9.8\n",
    "# langchain              = 0.0.339\n",
    "# numexpr                = 2.8.7\n",
    "# openai                 = 1.3.4\n",
    "# pydantic               = 2.5.2\n",
    "# python-dotenv          = 1.0.0\n",
    "# pytube                 = 15.0.0\n",
    "# tiktoken               = 0.5.1\n",
    "# youtube-transcript-api = 0.6.1\n",
    "# pypdfium2              = 4.24.0\n",
    "# wikipedia              = 1.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd3dc66-ad22-4ab5-b07e-0159ec8935e7",
   "metadata": {},
   "source": [
    "## <font color=blue>Keep API Key Safe</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b90086c8-5dc7-4948-9fe1-3bba3ab04a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impport libraries\n",
    "\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2f4a88b3-bb7d-490c-b51e-72a1129dbb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the env.txt that contains the OPENAI key\n",
    "# env.txt contains:\n",
    "# OPENAI_API_KEY=“sk-1234567890…………………..”\n",
    "\n",
    "load_dotenv('env.txt')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda5fa9f-91eb-4ecf-9b18-7f87066e9c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the key \n",
    "#print(openai.api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27b62f4-5ae7-4957-816b-82c1fdbc25e5",
   "metadata": {},
   "source": [
    "## Setup Test #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82539c18-3f7a-4470-881e-0f4406d910da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# load OPENAI API key\n",
    "load_dotenv('env.txt')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1149243e-214f-4d01-9561-ef1750be7f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load langchain libraries\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3c2d49-897d-46e2-ba34-e49ce84a897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise ChatModel with API key\n",
    "chat_model = ChatOpenAI(openai_api_key=openai_api_key)\n",
    "\n",
    "# setup message prompt\n",
    "text = \"What date is Singapore National Day?\"\n",
    "messages = [HumanMessage(content=text)]\n",
    "\n",
    "# ask ChatGPT\n",
    "print(chat_model.invoke(messages).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e079acd-9f31-42de-a5be-3a6306903480",
   "metadata": {},
   "source": [
    "## Setup Test #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6c78d8-416b-40ae-b37d-2520e1a20ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code to read in the OpenAI API key is skipped here\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaf5218-8d48-4403-9d1e-fa5fe3fc2887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise ChatModel with API key\n",
    "chat_model = ChatOpenAI(openai_api_key=openai_api_key, model_name='gpt-3.5-turbo', temperature=0.3)\n",
    "\n",
    "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "human_template = \"{text}\"\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "\n",
    "# trsnslate English to French\n",
    "print(chat_model(\n",
    "        chat_prompt.format_messages(\n",
    "            input_language=\"English\", \n",
    "            output_language=\"French\", \n",
    "            text=\"I love programming.\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "print()\n",
    "\n",
    "# translate English to Chinese\n",
    "print(chat_model(\n",
    "        chat_prompt.format_messages(\n",
    "            input_language=\"English\", \n",
    "            output_language=\"Chinese\", \n",
    "            text=\"I love programming.\"\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f79bf6-b3ec-41f0-9346-3ef1830af691",
   "metadata": {},
   "source": [
    "## <font color=blue>Models</font>\n",
    "LangChain provides two types of models\n",
    "* LLMs\n",
    "* ChatModels\n",
    "\n",
    "Let's take a look at LLMs first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659b8de5-45c9-41fd-bdcc-01f420b5ad81",
   "metadata": {},
   "source": [
    "### LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c43476-5782-4002-8243-14bba9ead93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# notice the model_name is 'gpt-3.5-turbo-instruct'\n",
    "llm = OpenAI(\n",
    "    model_name='gpt-3.5-turbo-instruct',\n",
    "    temperature=0.7,\n",
    "    max_tokens=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d07b0e-63af-45a6-9373-7c0b0b174ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(\"Explain what is generative AI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c21625-821a-465a-b031-e466a57e8093",
   "metadata": {},
   "source": [
    "### ChatModels\n",
    "* LLMs under the hood\n",
    "* Specific to chatbots\n",
    "* 3 major components\n",
    "  - HumanMessage\n",
    "  - AIMessage\n",
    "  - SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bc8a3e-08f0-4cc2-a690-df0f59d224ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [SystemMessage(\n",
    "    content=\"You are a Grammar Teacher who responds Yes for correct Grammar input\"),\n",
    "    HumanMessage(content=\"I love, programming.\")]\n",
    "\n",
    "# notice the model name is 'gpt-3.5-turbo'\n",
    "chat = ChatOpenAI(\n",
    "    model_name='gpt-3.5-turbo'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302a4c15-296a-479d-b132-59eff2e8aef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat(messages).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d867b4-2189-4418-b20a-0773a25f556c",
   "metadata": {},
   "source": [
    "## <font color=blue>Prompt Template</font>\n",
    "Create a prompt from string input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5486900e-03eb-4189-bade-39b8c9443ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import LLMChain\n",
    "\n",
    "# define the template\n",
    "template=\"Write {lines} lines about {topic}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc680d9-43cf-4dca-82a9-e3d378a30bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the LLM\n",
    "\n",
    "llm=OpenAI(\n",
    "    model_name='gpt-3.5-turbo-instruct',\n",
    "    temperature=0.7,\n",
    "    openai_api_key=openai_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcc93ac-6d75-465c-9810-edf3b7e2a61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup prompt template\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "   template=template, \n",
    "   input_variables=[\"lines\",\"topic\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db22bd96-fff4-4e28-9333-d1a14d0e0269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise lines and topic\n",
    "lines=\"3\"\n",
    "topic=\"Sir Stamford Raffles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5175acb-daeb-4a99-8774-6e2a8f3bb754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the chain\n",
    "\n",
    "llm_chain.run(lines=lines, topic=topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30450c6-0d98-4864-a0e1-0ddb2c5235a4",
   "metadata": {},
   "source": [
    "### Alternative code \n",
    "Using LangChain Expression Language (LCEL) - Pipe operator '|'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcd25c3-2d99-48d2-94fb-1588008e7ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\n",
    "   'lines':'3', \n",
    "   'topic':'Sir Stamford Raffles'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d2a7a1-d5d3-4f3a-9ec9-23829cf6efe4",
   "metadata": {},
   "source": [
    "## ChatPromptTemplate\n",
    "* PromptTemplate are for LLM models\n",
    "* ChatPromptTemplate are for ChatModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cf3613-2797-4869-bdce-be82d3f7904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts.chat import SystemMessage, HumanMessagePromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcba806-3abb-4767-8f41-9292669069b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\"You are a python coder that helps user with writing programs\")),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b39bd61-2a55-4654-a414-5b89b17be2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf754508-48a3-44fd-af96-2eb39b8dd54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm(template.format_messages(text='Check whether a number is prime or not')).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238b22e4-02b1-49e3-98a7-28f8ba41c16d",
   "metadata": {},
   "source": [
    "## <font color=blue>OutputParsers</font>\n",
    "* Not only you can customise your prompt, you can set formats for the output as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e675b9c-0d7a-4809-844a-1b77c38a7e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3702497-f35d-4ff8-bf65-9b3fd347f18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"List down 5 countries that start with letter'{alphabet}'\\n{format_instructions}\",\n",
    "    input_variables=[\"alphabet\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ac4fc8-2427-4375-ac3d-b209b46ad50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(format_instructions)\n",
    "# print()\n",
    "# print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de5724d-9743-4011-a2cf-88651963f6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(\n",
    "    temperature=0, \n",
    "    openai_api_key=openai_api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdd8543-643f-4288-b0d1-2e065f514564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to see the default parameters of OpenAI\n",
    "\n",
    "# print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15101a2a-d149-446a-abe4-d1c5db06d995",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputp = prompt.format(alphabet=\"S\")\n",
    "print(inputp)\n",
    "\n",
    "output = llm(inputp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b51ab4-a970-4baf-8af9-558603621722",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac04023b-08a0-4fcf-98f5-ac1e01822a39",
   "metadata": {},
   "source": [
    "## Pydantic Model\n",
    "\n",
    "* https://betterprogramming.pub/how-to-add-natural-language-input-to-an-existing-python-application-with-langchain-and-pydantic-7774048c7ab7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97fe78c-b9a0-4950-a668-76d9b3ba3610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, conlist\n",
    "from typing import List, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9120bace-d08d-4ba5-8762-60bd42f30322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic model to describe the structure of the data we want to parse. In this case, \n",
    "# we want to capture information about a user’s music taste, including the genres, bands, \n",
    "# and albums they like. Here’s how to define such a model:\n",
    "\n",
    "class MusicTasteDescriptionResult(BaseModel):\n",
    "    genres: Optional [conlist(str, min_length=1, max_length=5)] = \\\n",
    "       Field(None, description=\"Music genres liked by the user. Must contain between 1 and 5 genres\")\n",
    "    \n",
    "    bands: Optional [conlist(str, min_length=1, max_length=5)] = \\\n",
    "       Field(None, description=\"Specific bands or artists liked by the user. If provided, must contain between 1 and 5 bands or artists\")\n",
    "    \n",
    "    albums: Optional [conlist(str, min_length=1, max_length=5)] = \\\n",
    "       Field(None, description=\"Specific albums liked by the user. If provided, must contain between 1 and 5 albums\")\n",
    "    \n",
    "    year_range: Optional [conlist(int, min_length=2, max_length=2)] = \\\n",
    "       Field(None, description=\"Year range of music liked by the user. If provided, must contain exactly 2 years indicating the start and end of the range\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea41fd9-0616-4969-a599-faee90cd4586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrate Pydantic with LangChain\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=MusicTasteDescriptionResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af01de8c-b3e8-4cfd-bdf5-7204b1d137dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some examples to guide LLM in outputting the expected object\n",
    "\n",
    "# We need to add .replace(\"{\", \"{{\").replace(\"}\", \"}}\") after serialising as JSON so that the curly brackets \n",
    "# in the JSON won’t be mistaken for prompt variables by LangChain. This piece simply escapes those curly brackets.\n",
    "examples = [\n",
    "    {\n",
    "        \"music taste description\": \"I like rock such as Rolling Stones or The Ramones, or the album London Calling from the clash\",\n",
    "        \"result\": MusicTasteDescriptionResult.parse_obj({\n",
    "            \"genres\": [\"rock\"],\n",
    "            \"bands\": [\"Rolling Stones\", \"The Ramones\", \"The Clash\"],\n",
    "            \"albums\": [\"London Calling\"]\n",
    "        }).json().replace(\"{\", \"{{\").replace(\"}\", \"}}\"),\n",
    "    },\n",
    "    {\n",
    "        \"music taste description\": \"I enjoy rock music from the 70s like Led Zeppelin\",\n",
    "        \"result\": MusicTasteDescriptionResult.parse_obj({\n",
    "            \"genres\": [\"rock\"],\n",
    "            \"bands\": [\"Led Zeppelin\"],\n",
    "            \"year_range\": [1970, 1979]\n",
    "        }).json().replace(\"{\", \"{{\").replace(\"}\", \"}}\"),\n",
    "    },\n",
    "    # add more examples as needed...\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca8bcb0-1a3a-4e4c-b48d-11000bedac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"music taste description\", \"result\"], \n",
    "    template=\"Query: {music taste description}\\nResult:\\n{result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59aae33-e0b4-4587-a486-a30b7a3c55be",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt, \n",
    "    prefix=\"\"\"Given a query describing a user's music taste, transform it into a structured object. {format_instructions}\"\"\",\n",
    "    suffix=\"Query: {input}\\nResult:\\n\", \n",
    "    input_variables=[\"input\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3bd763-9c26-4d37-844e-42319a0cbbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call .format() to generate the string, and pass the input variables as keyword arguments\n",
    "print(example_prompt.format(**examples[0])) \n",
    "\n",
    "print(\"\\n#######\\n\")\n",
    "\n",
    "print(prompt.format(input=\"My favorite band is The Beatles\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9633e7-4cdb-44a8-9d57-84537faaf726",
   "metadata": {},
   "source": [
    "## Running the Chain\n",
    "\n",
    "* More details are convered in the **Chains** section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6198262-b98f-48cf-8739-38f035929fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3efdc8-29b4-4651-8e21-e22ea58ae798",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.run(\"I love pop music from the 80s, especially Madonna\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7116364e-7acb-4f20-b06f-89e809f0baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    parsed_taste = parser.parse(output)\n",
    "    print(f\"\"\"\n",
    "Genres: {\", \".join(parsed_taste.genres) if parsed_taste.genres else 'Not specified'}\n",
    "Bands: {\", \".join(parsed_taste.bands) if parsed_taste.bands else 'Not specified'}\n",
    "Albums: {\", \".join(parsed_taste.albums) if parsed_taste.albums else 'Not specified'}\n",
    "Year Range: {f\"{parsed_taste.year_range[0]} - {parsed_taste.year_range[1]}\" if parsed_taste.year_range else 'Not specified'}\n",
    "    \"\"\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60525205-66aa-4368-a686-f2ff4fa0c7e9",
   "metadata": {},
   "source": [
    "## <font color=blue>Chains</font>\n",
    "* Using an LLM in isolation is fine for simple applications\n",
    "* More complex applications require chaining LLMs\n",
    "* Either with each other or with other components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c5bb62-9e77-4288-a61e-74d9846ee070",
   "metadata": {},
   "source": [
    "### Basic Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf6b306-76a5-455f-af75-628147f62c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5e2af0-606f-413b-b905-9943fb6efc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"city\"],\n",
    "    template=\"Describe a perfect day in {city}?\",\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589b0c2e-dd58-43d3-919b-ec9a5d33cc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(\n",
    "    temperature=0, \n",
    "    openai_api_key=openai_api_key,\n",
    ")\n",
    "\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d1cc27-aea7-4347-b44e-d03add7cbf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llmchain = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "llmchain.run(\"Paris\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71995376-0cf2-4abe-b098-4108ed5e3d2c",
   "metadata": {},
   "source": [
    "### LLM Math Chain\n",
    "* Designed to solve complex word math problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30869090-77b3-4988-9914-6312ac74f868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "What is 13 raised to the .3432 power?\u001b[32;1m\u001b[1;3m\n",
      "```text\n",
      "13**.3432\n",
      "```\n",
      "...numexpr.evaluate(\"13**.3432\")...\n",
      "\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m2.4116004626599237\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Answer: 2.4116004626599237'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMMathChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# verbose = False\n",
    "# Answer: 2.4116004626599237\n",
    "#\n",
    "# verbose = True\n",
    "# > Entering new LLMMathChain chain...\n",
    "# What is 13 raised to the .3432 power?\n",
    "# ```text\n",
    "# 13**.3432\n",
    "# ```\n",
    "# ...numexpr.evaluate(\"13**.3432\")...\n",
    "#\n",
    "# Answer: 2.4116004626599237\n",
    "# > Finished chain.\n",
    "# 'Answer: 2.4116004626599237'\n",
    "llm_math = LLMMathChain.from_llm(\n",
    "    llm, \n",
    "    verbose=True\n",
    ")  \n",
    "\n",
    "llm_math.run(\"What is 13 raised to the .3432 power?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482d1c8f-c211-4893-a0bf-134e8ef9839e",
   "metadata": {},
   "source": [
    "### More Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60748b5c-267d-4570-91a3-d66331945471",
   "metadata": {},
   "source": [
    "### Simple Sequential Chain\n",
    "* Single input/output\n",
    "* If more than 1 input/output, error is \"Chains used in SimplePipeline should all have one input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1be0462d-eb61-4280-bbda-58035167375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "29e467c8-299c-4450-99ed-382310a99131",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    temperature=0.9,  # more creative\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7286bb3d-42bc-443b-a72a-23e0bee5b03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe a company that makes {product}?\"\n",
    ")\n",
    "\n",
    "chain_one = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt = first_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a7a151a9-bee6-4b5d-b6b0-77f97138287b",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a 20 words description for the following company: {company}.\"\n",
    ")\n",
    "\n",
    "chain_two = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt = second_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "131c6c93-6925-4a0c-96da-5ad0bec6a055",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_simple_seq_chain = SimpleSequentialChain(\n",
    "    chains=[chain_one, chain_two],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6a914362-9e8c-4863-baf0-721de99c658a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mStrideTech\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mStrideTech is a tech company specializing in innovative solutions and software for optimizing and enhancing athletic performance and coaching.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'StrideTech is a tech company specializing in innovative solutions and software for optimizing and enhancing athletic performance and coaching.'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product = \"running shoes\"\n",
    "\n",
    "overall_simple_seq_chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c81b99-9f55-4076-bd3c-29cb7d9d895d",
   "metadata": {},
   "source": [
    "### Sequential Chain\n",
    "* Multiple inputs/outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f74d954a-b4c3-4237-b8f0-ef454b64d166",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e2e1de02-37f1-427e-9154-407243c78fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 1: translate to english\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe a company that makes {product}?\",    \n",
    ")\n",
    "\n",
    "chain_one = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=first_prompt, \n",
    "    output_key=\"output_first_chain\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "78bb57d1-dc30-4490-be98-957a3e502fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a 20 words description for the {output_first_chain} that makes best in class {product}.\"\n",
    ")\n",
    "\n",
    "chain_two = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt = second_prompt,\n",
    "    output_key = \"output_second_chain\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "30f07c1e-94ba-49ad-bc5d-4a00259cbb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_seq_chain = SequentialChain(\n",
    "    chains = [chain_one, chain_two],\n",
    "    input_variables=[\"product\"],\n",
    "    output_variables=[\"output_second_chain\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ef06b942-b68d-4690-9036-34286293aaed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'StrideTech: A leading running shoe company that blends cutting-edge technology with superior craftsmanship to create top-tier footwear.'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product = \"running shoe\"\n",
    "\n",
    "overall_seq_chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb24f7c-7507-4e99-a1ea-6b60646a7930",
   "metadata": {},
   "source": [
    "### Router Chain\n",
    "* Create a chain that dynamically selects a single chain out of a series of other chains to use, depending on the user input or the prompt provided.\n",
    "* Contains two main things:\n",
    "  * RounterChain: responsible for selecting the next chain to call\n",
    "  * destination_chains: chains that the router chain can route to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "39363643-0e72-4b85-9109-8e20dc8831bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# LLM Router chains\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3056c4-df66-410e-bc7c-7b95b5ad0cab",
   "metadata": {},
   "source": [
    "### Creating And Mapping Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "26e4a827-4871-4671-88c0-488526ca8f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_teacher = \"\"\"Your name is Mary, you are a Math teacher at a \\\n",
    "primary school. You are very good at teaching math due to your ability \\\n",
    "to breakdown complicated tasks into much smaller ones. \\\n",
    "Students ask different questions about math, you are responsible to answer them. \\\n",
    "Use your solid mathematics skills to explain the concepts in an easy to understand manner. \\\n",
    "All your responses should start in the format below:\n",
    "Hello,\n",
    "          \n",
    "Below is a question from a student in your class:\n",
    "{input}\\\n",
    "\"\"\"\n",
    "\n",
    "science_teacher = \"\"\"Your name is Susan, you are a science teacher at a \\\n",
    "primary school. You are very good at teaching science due to \\\n",
    "your ability to explain help students develop exploratory questions, develop hypotheses \\\n",
    "to explain natural events, and encourage students to test and refine their explanations based on scientific evidence. \\\n",
    "Students ask different questions about science, you are responsible to answer them \\\n",
    "Use your scientific thinking skills to explain the concepts in very easy to understand manner. \\\n",
    "All your responses should start in the format below:\n",
    "Hello,\n",
    "         \n",
    "Below is a question from a student in your class:\n",
    "{input}\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f895d0a5-19c4-45c8-8e9e-f5860a15cb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"Math Teacher\",\n",
    "        \"description\": \"Good for answering questions about Math\",\n",
    "        \"prompt_template\": math_teacher,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Science Teacher\",\n",
    "        \"description\": \"Good for answering questions about Science\",\n",
    "        \"prompt_template\": science_teacher,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3808f385-6114-4a7f-9e6b-3753d3f344c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6afc33-6a50-40ce-b3e7-d4df06ce8309",
   "metadata": {},
   "source": [
    "### Mapping Destination Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "67d27734-3f63-4b40-84cc-a68f16175911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map destination chains\n",
    "destination_chains = {}\n",
    "\n",
    "for prompt_info in prompt_infos:\n",
    "    name = prompt_info[\"name\"]\n",
    "    prompt_template = prompt_info[\"prompt_template\"]\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"input\"])\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d24e06c5-0990-46e1-bd31-bc9b087fb8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b7b9bfc8-f04b-44c1-af68-926678c0a55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Math Teacher: Good for answering questions about Math\n",
      "Science Teacher: Good for answering questions about Science\n"
     ]
    }
   ],
   "source": [
    "print(destinations_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4725ee43-427a-4ccb-87e1-c09c6f69e5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "eaf2fe03-3fb0-4ff8-aef0-8dda5ec57ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "935aaeab-f526-415f-a3f9-5bb7b38ab328",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb1b050-f50e-48f6-b745-0a935cb7983d",
   "metadata": {},
   "source": [
    "### Create Router Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f866cf2c-25b7-4ffa-ae2e-1d72f5d990c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the router chain\n",
    "router_chain = LLMRouterChain.from_llm(\n",
    "    llm, \n",
    "    router_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4a2594d6-fb09-4c4d-99ba-93e5ce03560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Prompt Chain\n",
    "chain = MultiPromptChain(\n",
    "    router_chain=router_chain,\n",
    "    destination_chains=destination_chains,\n",
    "    default_chain=default_chain,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db99e00f-f822-4a96-98d1-885784b26d63",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ce9e2da5-9f47-4b46-acd3-cba430fa0c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\koay_seng_tian\\AppData\\Local\\anaconda3\\envs\\llms\\lib\\site-packages\\langchain\\chains\\llm.py:321: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Math Teacher: {'input': 'What is the meaning of integration?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello,\\n\\nIntegration is a mathematical concept that involves finding the area under a curve. It is a way to calculate the total accumulation of a quantity over a given interval. \\n\\nTo understand integration, let's imagine you have a graph that represents the speed of a car over time. The graph shows how the car's speed changes at different moments. Integration allows us to determine the total distance traveled by the car during a specific time interval.\\n\\nIn simpler terms, integration helps us find the total amount or the sum of something. It can be used to calculate the total area of a shape, the total amount of water in a tank, or even the total amount of money spent over a period of time.\\n\\nTo perform integration, we use a mathematical tool called the integral symbol (∫) and a function that represents the curve we want to find the area under. By applying integration techniques, we can calculate the exact value of the area or the total accumulation.\\n\\nI hope this explanation helps you understand the meaning of integration. If you have any further questions, feel free to ask!\""
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"What is the meaning of integration?\")\n",
    "\n",
    "# notice the output is answered by \"Math Teacher\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1cea6484-782a-442d-9f3a-bfdfdb44ba46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\koay_seng_tian\\AppData\\Local\\anaconda3\\envs\\llms\\lib\\site-packages\\langchain\\chains\\llm.py:321: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Science Teacher: {'input': 'What is the basic theory of quantum physics?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello,\\n\\nThe basic theory of quantum physics is a branch of physics that explains how very tiny particles, like atoms and subatomic particles, behave. It is based on the idea that these particles can exist in multiple states at the same time, called superposition. \\n\\nIn quantum physics, particles can also be connected to each other in a special way called entanglement. This means that what happens to one particle can instantly affect another particle, no matter how far apart they are.\\n\\nAnother important concept in quantum physics is uncertainty. This means that we can't know both the exact position and the exact speed of a particle at the same time. It's like trying to take a picture of a moving object in the dark - you can either see where it is or how fast it's moving, but not both.\\n\\nQuantum physics is a very complex and fascinating field, and scientists are still learning more about it every day. I hope this helps you understand the basic theory of quantum physics!\\n\\nBest regards,\\nSusan\""
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"What is the basic theory of quantum physics?\")\n",
    "\n",
    "# notice the output is answered by \"Science Teacher\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f96192ab-01a9-486f-8ac8-5d1647ae45e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\koay_seng_tian\\AppData\\Local\\anaconda3\\envs\\llms\\lib\\site-packages\\langchain\\chains\\llm.py:321: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None: {'input': \"Translate 'Singapore is a garden city' to japanese\"}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'シンガポールはガーデンシティです。 (Shingapōru wa gādenshiti desu.)'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"Translate 'Singapore is a garden city' to japanese\")\n",
    "\n",
    "# notice the output is answered by \"None\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c711e4-7209-42e7-8b63-e2e8dbf7e2ab",
   "metadata": {},
   "source": [
    "## <font color=blue>Memory</font>\n",
    "* LLMs are stateless. Each inbcoming query is processed independently of the other interactions.\n",
    "* * Memory allows a LLM to remember previous interactions with the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bab3a6-e235-4731-9841-c46666fdad72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# first initialize the large language model\n",
    "llm = OpenAI(\n",
    "\ttemperature=0,\n",
    "\topenai_api_key=openai.api_key,\n",
    "\t#model_name=\"text-davinci-003\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb9ebd6-d870-40c2-b4b8-c42aea023b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9df4cde-db4a-49ee-9369-ebb4f64b3b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now initialize the conversation chain\n",
    "conversation = ConversationChain(llm=llm)\n",
    "\n",
    "# print default prompt template\n",
    "# notice the prompt attempts to reduce hallucinations by stating:\n",
    "# \"If the AI does not know the answer to a question, it truthfully says it does not know.\"\n",
    "print(conversation.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e46fad-beb9-4f02-b849-470bed085801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConversationBufferMemory is the most straightforward conversational memory in LangChain\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "\n",
    "conversation_buf = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c4b6a6-f7fd-4e1c-964d-bbe5ebbb0886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start the conversation\n",
    "conversation_buf(\"Good morning AI!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5840b45-4750-4ac4-9c32-24795e1d1ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "# see how many tokens are being used by each interaction/conversation.\n",
    "def count_tokens(chain, query):\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "        print(str(cb) + '\\n')\n",
    "        #print(f'Spent a total of {cb.total_tokens} tokens')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221dddc0-1617-44a3-aa50-ee81e97d4d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tokens(\n",
    "    conversation_buf, \n",
    "    \"My interest here is to understand Large Language Models and then be able to explain its working to a 10 years old.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458c70eb-9b4f-4b04-8744-690bd9bd53fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tokens(\n",
    "    conversation_buf, \n",
    "    \"Can you suggest some practical examples of Large Language Model to help me in my explanation?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716f1fe7-a374-4482-9843-f7df79eaf84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tokens(\n",
    "    conversation_buf, \n",
    "    \"How do I explain 'text classification' to a 10 years old? Give some examples to help me explain.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42cbe10-8c1b-401b-ab79-21765579b58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tokens(\n",
    "    conversation_buf, \n",
    "    \"What is my aim again?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d7455c-b11a-47de-8afc-951c417c3e16",
   "metadata": {},
   "source": [
    "### ConversationBufferMemory\n",
    "\n",
    "From the above, we see that LLM can clearly remember the history of conversation. Let’s take a look at how this conversation history is stored by the ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7528f8-b262-423e-9d04-31c94abefe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conversation_buf.memory.buffer.replace('\\n', '\\n\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c01cf6-3329-4094-98b6-0a83a6c6fa74",
   "metadata": {},
   "source": [
    "## ConversationSummaryMemory\n",
    "Using **ConversationBufferMemory**, tokens will be used quickly during the exchanges.  It can exceed the context window limit of even the most advanced LLMs available today.\n",
    "\n",
    "To avoid excessive token usage, we can use ConversationSummaryMemory. As the name would suggest, this form of memory summarizes the conversation history before it is passed to the {history} parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0358afb9-0e75-4ced-8b7d-f0e148822623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationSummaryMemory\n",
    "\n",
    "conversation_sum = ConversationChain(\n",
    "\tllm=llm,\n",
    "\tmemory=ConversationSummaryMemory(llm=llm)\n",
    ")\n",
    "\n",
    "print(conversation.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f084f9e-1f28-4a6c-90dc-19500c126b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_sum(\"Good morning AI!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c7b91b-c5d9-4beb-9ebe-68eaf134ce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tokens(\n",
    "    conversation_sum, \n",
    "    \"My interest here is to understand Large Language Models and then be able to explain its working to a 10 years old.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5ba022-8757-4844-b15a-5808835381c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tokens(\n",
    "    conversation_sum, \n",
    "    \"Can you suggest some practical examples of Large Language Model to help me in my explanation?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1dff03-8de2-4e16-b522-a44b68b264ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tokens(\n",
    "    conversation_sum, \n",
    "    \"How do I explain 'text classification' to a 10 years old? Give some examples to help me explain.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc22afc-8f1a-4127-88a5-59612cb347f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tokens(\n",
    "    conversation_sum, \n",
    "    \"What is my aim again?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48bb2a5-a08c-48b6-bb1c-d352d0043156",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conversation_sum.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159fdb16-8916-4d45-bc79-0b245e4511cf",
   "metadata": {},
   "source": [
    "### ConversationBufferMemory vs ConversationBufferMemory\n",
    "The number of tokens being used for this conversation is greater than when using the ConversationBufferMemory, so is there any advantage to using ConversationSummaryMemory over the buffer memory?\n",
    "\n",
    "Yes, for long conversation.  \n",
    "\n",
    "[Source](https://github.com/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/03a-token-counter.ipynb)\n",
    "<img src=\"langchain_conversation_summary.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9d1c4a-fd92-4221-acb9-f4dc9cb06488",
   "metadata": {},
   "source": [
    "## <font color=blue>Retrieval</font>\n",
    "* Retrieve relevant information from an external data source and pass the information to LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca4ab35-22b6-4154-81ad-737b5c6ec12d",
   "metadata": {},
   "source": [
    "### Retrieve and Summarise a Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b952e18d-b7cd-4944-85b9-0fae007f7f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1460300f-bedb-4486-aed2-05c37d4fe65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Document loader\n",
    "loader = TextLoader(\"MicrosoftEULA.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e843025d-624f-4649-8865-3719278ab968",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Document Transoformer\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1450, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4c5491-dbe7-44aa-90dd-e796c489c64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embedding model\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6fcade-4ff1-456d-9bea-089d79039322",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vector DB to store embeddings\n",
    "docsearch = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434f7644-24ae-4dde-8078-d6e7caa6ec57",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_text_llm = OpenAI(openai_api_key=openai_api_key)\n",
    "\n",
    "print(qa_text_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c43c1ad-8bf9-4dcb-99d6-47cfc213c0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retriever being used using parameter retriever\n",
    "qa_text = RetrievalQA.from_chain_type(\n",
    "    llm=qa_text_llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=docsearch.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e387a162-ba52-46e9-853d-fbb5f74de6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What this document about? Summarize it in a paragraph\"\n",
    "\n",
    "print(qa_text.run(query).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13d13e4-34f2-4630-ba89-f88371b2d969",
   "metadata": {},
   "source": [
    "### Retrieve and Query a CSV File\n",
    "\n",
    "Check the source [**here**](https://github.com/Ryota-Kawamura/LangChain-for-LLM-Application-Development/blob/main/L4-QnA.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ebb0ed-0492-4fb8-96e3-d4950544730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034ae1ff-9b53-4061-a3eb-a79e7eab90be",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'OutdoorClothingCatalog_1000.csv'\n",
    "\n",
    "loader = CSVLoader(\n",
    "    file_path=file, \n",
    "    encoding='utf-8'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49a89a5-5aaf-47ea-8a64-173c28019d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch\n",
    ").from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7cec0e-2a6f-49e2-8d76-7926dfc5def0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query =\"Please list all your shirts with sun protection in a table in markdown and summarize each one.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa543f4a-1b4f-4f44-9df4-cc8f1ca2a92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = ChatOpenAI()\n",
    "\n",
    "qa_csv = RetrievalQA.from_chain_type(\n",
    "    llm = openai,\n",
    "    chain_type='stuff',\n",
    "    retriever = index.vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "response = qa_csv.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756ae714-2bdf-492f-a3fe-230701e93cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211f26c6-ac11-480b-a8b1-0401c7f65f17",
   "metadata": {},
   "source": [
    "### Retrieve and Query a YouTube Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf57a53-8bb7-40ac-b479-5ee4e731db28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import YoutubeLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0edaab-0c14-4720-aef6-aa4bc78b009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the youtube videos are about:\n",
    "# 1. The Best Explanation of Pi\n",
    "# 2. Cool Drone Arc Aerosystem Amazing Invention\n",
    "\n",
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    #\"https://www.youtube.com/watch?v=TlY-Sh9Rzas\", \n",
    "    \"https://www.youtube.com/watch?v=n5qcYwxyb7E\",\n",
    "    add_video_info=True,\n",
    "    language=[\"en\"]    \n",
    ")\n",
    "\n",
    "youtube_data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb03c0be-6468-466a-a126-389f9e68dbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(youtube_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c497c4-35f1-4842-87d1-d65734698986",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "youtube_text = text_splitter.split_documents(youtube_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d079fa-f1d4-47ab-85e4-12bedeff0456",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(youtube_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178f674f-3056-4c47-b4a9-22f0bae73c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embedding model\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c05ef6a-25e6-41ab-b649-38ba1217e9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_youtube_llm = OpenAI(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f4b258-d12e-4083-b450-db82c887f530",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = Chroma.from_documents(youtube_text, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9105b991-214e-46e1-975f-3186240347b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retriever being used using parameter retriever\n",
    "qa_youtube = RetrievalQA.from_chain_type(\n",
    "    llm=qa_youtube_llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=docsearch.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a64229-266d-46c7-ab22-57fa890e7234",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query = \"Summarize the text in short bullet-points\"   # for best explanation of pi\n",
    "query = \"What is so amazing about the invention?\"      # for Cool Drone Arc Aerosystem\n",
    "\n",
    "print(qa_youtube.run(query).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dc5b67-a68e-4438-ac8e-ddffa4e784c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ef44a3f-1992-47dd-9ee5-5cc2ded436dd",
   "metadata": {},
   "source": [
    "### Retrieve and Query a Text PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798322e8-4de7-40f6-b217-3d87b20ccf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import YoutubeLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFium2Loader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c019a777-2a8a-48a1-bf3d-5252c5b31f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFium2Loader(\"1706.03762.pdf\")\n",
    "pdf_data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d058b9ff-0a01-43ae-869c-92d5a1e09b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "pdf_text = text_splitter.split_documents(pdf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc39746e-e9e7-439d-b50d-c5ec6e347c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embedding model\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "\n",
    "qa_pdf_llm = OpenAI(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c919f32b-274e-4733-b1c2-41f1d3c2774b",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = Chroma.from_documents(pdf_text, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519d217e-c850-4878-9688-f9027779b07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retriever being used using parameter retriever\n",
    "qa_pdf = RetrievalQA.from_chain_type(\n",
    "    llm=qa_pdf_llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=docsearch.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a1140f-f25d-4ebb-99f7-520da4f9e90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Summarise the paper\"\n",
    "\n",
    "print(qa_pdf.run(query).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a8841b-6c1a-4abe-a53f-992244a1f64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Explain the workings of multi-attention head(s) to a computer undergraduate.\"\n",
    "\n",
    "print(qa_pdf.run(query).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744275c1-1ac3-48c2-903e-2a47e20290a0",
   "metadata": {},
   "source": [
    "## <font color=blue>Agent</font>\n",
    "* The core idea of agents is to use a language model to choose a sequence of actions to take. In chains, a sequence of actions is hardcoded (in code). In agents, a language model is used as a reasoning engine to determine which actions to take and in which order.\n",
    "\n",
    "### Tools\n",
    "* Tools are interfaces that an agent can use to interact with the world\n",
    "* For a list of available tools, refer to [LangChain-Agent-Tools](https://python.langchain.com/docs/integrations/tools/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b05b2296-16af-47b2-b883-ce7c5895abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "from langchain.tools import Tool, DuckDuckGoSearchRun, ArxivQueryRun, WikipediaQueryRun \n",
    "from langchain.utilities import WikipediaAPIWrapper \n",
    "\n",
    "from langchain.agents import initialize_agent \n",
    "from langchain.agents import AgentType \n",
    "from langchain.chat_models import ChatOpenAI \n",
    "from langchain.chains import LLMChain \n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d42cec41-9f15-4da2-b077-9b9e3694bb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the DuckDuckGo search tool along with Arxiv and Wikipedia\n",
    "\n",
    "search = DuckDuckGoSearchRun()  \n",
    "arxiv = ArxivQueryRun()  \n",
    "wiki = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe556e2-3644-46bb-8e6c-cefb6841c7b8",
   "metadata": {},
   "source": [
    "### Set Up Essay Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21263f97-6a60-4c69-bb38-9d3e1d0f13c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    temperature=0, \n",
    "    openai_api_key=openai_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e9ca63a-1e0f-4009-9d21-dac42301fd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"Write an essay for the topic provided by the user with the help of following content: {content}\"  \n",
    "\n",
    "essay = LLMChain(  \n",
    "    llm=llm,  \n",
    "    prompt=PromptTemplate.from_template(prompt_template)  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fce329bf-6ae2-4ac4-a261-ca9d0c0d264b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [  \n",
    "    Tool(  \n",
    "\t    name=\"Search\",  \n",
    "\t    func=search.run,  \n",
    "\t    description=\"Useful for when you need to answer questions about current events.\"  \n",
    "\t),  \n",
    "    \n",
    "\tTool(  \n",
    "\t    name=\"Arxiv\",  \n",
    "\t    func=arxiv.run,  \n",
    "\t    description=\"Useful when you need an answer about encyclopedic general knowledge\"  \n",
    "\t),  \n",
    "    \n",
    "\tTool(  \n",
    "\t    name=\"Wikipedia\",  \n",
    "\t    func=wiki.run,  \n",
    "\t    description=\"Useful when you need an answer about encyclopedic general knowledge\"  \n",
    "\t),  \n",
    "    \n",
    "\tTool.from_function(  \n",
    "\t    func=essay.run,  \n",
    "\t    name=\"Essay\",  \n",
    "\t    description=\"Useful when you need to write an essay\"  \n",
    "\t),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5eb90bde-b5fa-4f6c-969f-38fbc38e5f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the agent\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools, \n",
    "    llm, \n",
    "    agent=AgentType.OPENAI_FUNCTIONS, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8928856c-bf61-4c1c-8266-99e028dfea05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Essay` with `Essay on Global Warming – Causes and Solutions`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mGlobal warming is a pressing issue that has garnered significant attention in recent years. It refers to the long-term increase in Earth's average surface temperature due to human activities, primarily the emission of greenhouse gases. This essay will explore the causes of global warming and propose potential solutions to mitigate its effects.\n",
      "\n",
      "One of the main causes of global warming is the burning of fossil fuels such as coal, oil, and natural gas. These fuels release carbon dioxide (CO2) and other greenhouse gases into the atmosphere, trapping heat and causing the planet to warm. The increased use of these fuels for transportation, electricity generation, and industrial processes has led to a significant rise in greenhouse gas emissions.\n",
      "\n",
      "Deforestation is another major contributor to global warming. Trees absorb CO2 from the atmosphere and act as carbon sinks. However, widespread deforestation, primarily for agriculture, results in the release of stored carbon back into the atmosphere. Additionally, the loss of trees reduces the planet's capacity to absorb CO2, exacerbating the greenhouse effect.\n",
      "\n",
      "Another significant cause of global warming is the release of methane, a potent greenhouse gas, from various sources. Methane is emitted during the production and transport of coal, oil, and natural gas. It is also released by livestock and other agricultural practices, as well as by the decay of organic waste in landfills.\n",
      "\n",
      "To address the issue of global warming, several solutions can be implemented. Firstly, transitioning to renewable energy sources such as solar, wind, and hydroelectric power can significantly reduce greenhouse gas emissions. Governments and individuals should invest in renewable energy infrastructure and promote its adoption through incentives and subsidies.\n",
      "\n",
      "Furthermore, efforts to reduce deforestation and promote reforestation are crucial in combating global warming. Governments should enforce stricter regulations on logging and land conversion for agriculture. Additionally, initiatives should be undertaken to plant trees and restore degraded forests, enhancing the planet's capacity to absorb CO2.\n",
      "\n",
      "Reducing methane emissions is also essential in mitigating global warming. Improved agricultural practices, such as better waste management and the use of methane digesters, can significantly reduce methane emissions from livestock and organic waste. Additionally, stricter regulations on methane leakage from oil and gas production can help curb emissions.\n",
      "\n",
      "Lastly, raising awareness and educating the public about the importance of reducing greenhouse gas emissions is vital. Individuals can make a difference by adopting sustainable practices such as conserving energy, reducing waste, and choosing environmentally friendly transportation options. Governments and organizations should also invest in public campaigns to promote climate-friendly behaviors.\n",
      "\n",
      "In conclusion, global warming is a pressing issue with severe consequences for our planet. The burning of fossil fuels, deforestation, and methane emissions are the primary causes of this phenomenon. However, by transitioning to renewable energy sources, reducing deforestation, curbing methane emissions, and raising awareness, we can work towards mitigating the effects of global warming and preserving the Earth for future generations.\u001b[0m\u001b[32;1m\u001b[1;3mGlobal warming is a pressing issue that has garnered significant attention in recent years. It refers to the long-term increase in Earth's average surface temperature due to human activities, primarily the emission of greenhouse gases. This essay will explore the causes of global warming and propose potential solutions to mitigate its effects.\n",
      "\n",
      "One of the main causes of global warming is the burning of fossil fuels such as coal, oil, and natural gas. These fuels release carbon dioxide (CO2) and other greenhouse gases into the atmosphere, trapping heat and causing the planet to warm. The increased use of these fuels for transportation, electricity generation, and industrial processes has led to a significant rise in greenhouse gas emissions.\n",
      "\n",
      "Deforestation is another major contributor to global warming. Trees absorb CO2 from the atmosphere and act as carbon sinks. However, widespread deforestation, primarily for agriculture, results in the release of stored carbon back into the atmosphere. Additionally, the loss of trees reduces the planet's capacity to absorb CO2, exacerbating the greenhouse effect.\n",
      "\n",
      "Another significant cause of global warming is the release of methane, a potent greenhouse gas, from various sources. Methane is emitted during the production and transport of coal, oil, and natural gas. It is also released by livestock and other agricultural practices, as well as by the decay of organic waste in landfills.\n",
      "\n",
      "To address the issue of global warming, several solutions can be implemented. Firstly, transitioning to renewable energy sources such as solar, wind, and hydroelectric power can significantly reduce greenhouse gas emissions. Governments and individuals should invest in renewable energy infrastructure and promote its adoption through incentives and subsidies.\n",
      "\n",
      "Furthermore, efforts to reduce deforestation and promote reforestation are crucial in combating global warming. Governments should enforce stricter regulations on logging and land conversion for agriculture. Additionally, initiatives should be undertaken to plant trees and restore degraded forests, enhancing the planet's capacity to absorb CO2.\n",
      "\n",
      "Reducing methane emissions is also essential in mitigating global warming. Improved agricultural practices, such as better waste management and the use of methane digesters, can significantly reduce methane emissions from livestock and organic waste. Additionally, stricter regulations on methane leakage from oil and gas production can help curb emissions.\n",
      "\n",
      "Lastly, raising awareness and educating the public about the importance of reducing greenhouse gas emissions is vital. Individuals can make a difference by adopting sustainable practices such as conserving energy, reducing waste, and choosing environmentally friendly transportation options. Governments and organizations should also invest in public campaigns to promote climate-friendly behaviors.\n",
      "\n",
      "In conclusion, global warming is a pressing issue with severe consequences for our planet. The burning of fossil fuels, deforestation, and methane emissions are the primary causes of this phenomenon. However, by transitioning to renewable energy sources, reducing deforestation, curbing methane emissions, and raising awareness, we can work towards mitigating the effects of global warming and preserving the Earth for future generations.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Global warming is a pressing issue that has garnered significant attention in recent years. It refers to the long-term increase in Earth's average surface temperature due to human activities, primarily the emission of greenhouse gases. This essay will explore the causes of global warming and propose potential solutions to mitigate its effects.\n",
      "\n",
      "One of the main causes of global warming is the burning of fossil fuels such as coal, oil, and natural gas. These fuels release carbon dioxide (CO2) and other greenhouse gases into the atmosphere, trapping heat and causing the planet to warm. The increased use of these fuels for transportation, electricity generation, and industrial processes has led to a significant rise in greenhouse gas emissions.\n",
      "\n",
      "Deforestation is another major contributor to global warming. Trees absorb CO2 from the atmosphere and act as carbon sinks. However, widespread deforestation, primarily for agriculture, results in the release of stored carbon back into the atmosphere. Additionally, the loss of trees reduces the planet's capacity to absorb CO2, exacerbating the greenhouse effect.\n",
      "\n",
      "Another significant cause of global warming is the release of methane, a potent greenhouse gas, from various sources. Methane is emitted during the production and transport of coal, oil, and natural gas. It is also released by livestock and other agricultural practices, as well as by the decay of organic waste in landfills.\n",
      "\n",
      "To address the issue of global warming, several solutions can be implemented. Firstly, transitioning to renewable energy sources such as solar, wind, and hydroelectric power can significantly reduce greenhouse gas emissions. Governments and individuals should invest in renewable energy infrastructure and promote its adoption through incentives and subsidies.\n",
      "\n",
      "Furthermore, efforts to reduce deforestation and promote reforestation are crucial in combating global warming. Governments should enforce stricter regulations on logging and land conversion for agriculture. Additionally, initiatives should be undertaken to plant trees and restore degraded forests, enhancing the planet's capacity to absorb CO2.\n",
      "\n",
      "Reducing methane emissions is also essential in mitigating global warming. Improved agricultural practices, such as better waste management and the use of methane digesters, can significantly reduce methane emissions from livestock and organic waste. Additionally, stricter regulations on methane leakage from oil and gas production can help curb emissions.\n",
      "\n",
      "Lastly, raising awareness and educating the public about the importance of reducing greenhouse gas emissions is vital. Individuals can make a difference by adopting sustainable practices such as conserving energy, reducing waste, and choosing environmentally friendly transportation options. Governments and organizations should also invest in public campaigns to promote climate-friendly behaviors.\n",
      "\n",
      "In conclusion, global warming is a pressing issue with severe consequences for our planet. The burning of fossil fuels, deforestation, and methane emissions are the primary causes of this phenomenon. However, by transitioning to renewable energy sources, reducing deforestation, curbing methane emissions, and raising awareness, we can work towards mitigating the effects of global warming and preserving the Earth for future generations.\n"
     ]
    }
   ],
   "source": [
    "# running the agent\n",
    "# define a prompt for the agent and run it\n",
    "# the agent will search web, Arxiv and Wikipedia for information on global warming\n",
    "# fetch the content and write an eassy\n",
    "\n",
    "prompt = \"Write an essay in 1000 words for the topic {input}, use the tools to retrieve the necessary information\"  \n",
    "input = \"Essay on Global Warming – Causes and Solutions\"  \n",
    "  \n",
    "print(agent.run(prompt.format(input=input)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c274d2-2d29-4d82-a957-39cb1b3e10bf",
   "metadata": {},
   "source": [
    "## <font color=blue>Callbacks</font>\n",
    "* Provides a callbacks system that allows you to hook into the various stages of your LLM application. \n",
    "* This is useful for logging, monitoring, streaming, and other tasks.\n",
    "* Built-in callbacks integrations with 3rd-party tools can be found [**here**](https://python.langchain.com/docs/integrations/callbacks/).\n",
    "* LangChain provides a few built-in handlers that are available in the langchain/callbacks module.\n",
    "* The most basic handler is the **StdOutCallbackHandler** which logs all events to **stdout**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36600cf0-9252-4c00-ae74-411cfaadd46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import AgentAction, AgentFinish, BaseMessage, LLMResult\n",
    "\n",
    "\n",
    "from typing import Dict, Any, List, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba324485-e852-4cb6-8830-d7273fd5b751",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseCallbackHandler:\n",
    "    \"\"\"Base callback handler that can be used to handle callbacks from langchain.\"\"\"\n",
    "\n",
    "    def on_llm_start(\n",
    "        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when LLM starts running.\"\"\"\n",
    "\n",
    "    def on_chat_model_start(\n",
    "        self, serialized: Dict[str, Any], messages: List[List[BaseMessage]], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when Chat Model starts running.\"\"\"\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run on new LLM token. Only available when streaming is enabled.\"\"\"\n",
    "\n",
    "    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run when LLM ends running.\"\"\"\n",
    "\n",
    "    def on_llm_error(\n",
    "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when LLM errors.\"\"\"\n",
    "\n",
    "    def on_chain_start(\n",
    "        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when chain starts running.\"\"\"\n",
    "\n",
    "    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> Any:\n",
    "        \"\"\"Run when chain ends running.\"\"\"\n",
    "\n",
    "    def on_chain_error(\n",
    "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when chain errors.\"\"\"\n",
    "\n",
    "    def on_tool_start(\n",
    "        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when tool starts running.\"\"\"\n",
    "\n",
    "    def on_tool_end(self, output: str, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run when tool ends running.\"\"\"\n",
    "\n",
    "    def on_tool_error(\n",
    "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when tool errors.\"\"\"\n",
    "\n",
    "    def on_text(self, text: str, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run on arbitrary text.\"\"\"\n",
    "\n",
    "    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run on agent action.\"\"\"\n",
    "\n",
    "    def on_agent_finish(self, finish: AgentFinish, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run on agent end.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f25f9e-f5aa-485a-83f2-ce472ee34507",
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = StdOutCallbackHandler()\n",
    "llm = OpenAI()\n",
    "prompt = PromptTemplate.from_template(\"1 + {number} = \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "73401da3-d1bf-4bc3-8219-5cb809650af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m1 + 2 = \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n3'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constructor callback: First, let's explicitly set the StdOutCallbackHandler when initializing our chain\n",
    "chain = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=prompt, \n",
    "    callbacks=[handler]\n",
    ")\n",
    "\n",
    "chain.run(number=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ac2e73b7-79be-42ff-b546-ed2822d488d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m1 + 2 = \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n3'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use verbose flag: Then, let's use the `verbose` flag to achieve the same result\n",
    "chain = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=prompt, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.run(number=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1f3c683a-98dc-4716-8602-40647d61e76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m1 + 2 = \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n3'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Request callbacks: Finally, let's use the request `callbacks` to achieve the same result\n",
    "chain = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "chain.run(number=2, callbacks=[handler])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5f42c9-afe8-482b-805f-8e9b2d065b5b",
   "metadata": {},
   "source": [
    "### Callbacks argument\n",
    "\n",
    "The **callbacks** argument is available on most objects throughout the API in two different places.\n",
    "* Constructor callbacks\n",
    "* Request callbacks\n",
    "\n",
    "Check [**here**](https://python.langchain.com/docs/modules/callbacks/) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d07d7d-1eeb-4d70-8dce-5ed2b97712dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
